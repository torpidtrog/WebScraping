{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6ddd0c",
   "metadata": {},
   "source": [
    "### Description of packages to run\n",
    "\n",
    "1. <b>bs4</b> has the BeautifulSoup package that is used for web scraping <br>\n",
    "2. <b>requests</b> garners information from a particular website <br>\n",
    "3. <b>time</b> is used to ascertain the time the code is running for a particular link <br>\n",
    "4. <b>sleep</b> suspends (waits) execution of the current thread for a given number of seconds <br>\n",
    "5. <b>sys</b> is used for exception handling <br>\n",
    "6. <b>Urllib</b> package is the URL handling module for python. It is used to fetch URLs (Uniform Resource Locators) it uses the <b>urlopen</b> function and is able to fetch URLs using a variety of different protocols <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from time import sleep\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from random import randint\n",
    "import re\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf2218",
   "metadata": {},
   "source": [
    "<b> Web scraping Intro </b> <br>\n",
    "\n",
    "Webscraping involves extraction information from the html texts, the specifics of which are embedded in certain classes that come under some tags. For instance 'div' is a tag, under 'div' lies the 'class' = 'author date' which will give us the date when the article was published <br>\n",
    "\n",
    "<b> Exception Handling </b> <br>\n",
    "\n",
    "Sometimes there can be errors such as 'NoneType' object has no attribute 'text', to avoid such errors to stop the code \n",
    "to execute completely, try and except function is used. The code to be run within each url is embedded under 'try' and if\n",
    "there are any exceptions then it passes through 'except', which bypasses the error to contine the code execution. <br>\n",
    "\n",
    "<b> WARNING!!! </b> <br>\n",
    "VinylPulse is an exceptionally difficult website to scrape, it has issues with loading urls properly, may load on the \n",
    "phone but may not on the laptop. It also can detect webscraping effectively, hence to avoid getting blocked:<br>\n",
    "1. One must enter a code to mimic human behaviour, sleep ensures that the code stays on a particular url, the time is randomised using the 'randint' function. Time is in seconds. <br>\n",
    "2. Run the code chunk below for maximum 20 pages at a time. <br>\n",
    "3. If VPN is available, change the region after 2-3 tries <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffac8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to append details in the dataframe (mainly to garner the links for each collectible), with using try \n",
    "#and except to bypass any exceptions\n",
    "\n",
    "page = 1\n",
    "title = []\n",
    "category = []\n",
    "author = []\n",
    "date = []\n",
    "actions = []\n",
    "more_info = []\n",
    "\n",
    "while page != 50:\n",
    "    url = f\"https://www.vinylpulse.com/category/incoming/page/{page}\"\n",
    "    html_text = requests.get(url).text\n",
    "    sleep(randint(60,120))\n",
    "    soup = BeautifulSoup(html_text,'html', multi_valued_attributes = None)\n",
    "    collectibles = soup.find_all('article')\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for collectible in collectibles:\n",
    "            title.append(collectible.find('h2', class_ = 'entry-title').text)\n",
    "            author.append(collectible.find('div', class_ = 'author date').text.replace('By:',''))\n",
    "            date.append(collectible.find('div', {\"class\": \"date\"}).text)\n",
    "            actions.append(collectible.find('div', class_ = 'actions').text)\n",
    "            more_info.append(collectible.h2.a['href'])\n",
    "\n",
    "        page = page + 1\n",
    "        print(url)\n",
    "        print(len(collectibles))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(list(zip(title, category, author, date, actions, more_info)),\n",
    "               columns =['Title', 'Category', 'Author', 'Date', 'Social Media', 'Link'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bdd795",
   "metadata": {},
   "source": [
    "All of these dataframes should be stored in csv to be concatenated later (storing in csv is a better idea in case the extraction occurs over days) <br>\n",
    "\n",
    "<b> To store a csv file </b> <br> \n",
    "df.to_csv(r'C:/*File path*/*name of file*.csv', index=False) <br>\n",
    "\n",
    "<b> Then read these files </b> <br> \n",
    "df = pd.read_csv('C:/*File path*/*name of file*.csv')<br> \n",
    "df1 pd.read_csv('C:/*File path*/*name of file*.csv')<br> \n",
    "\n",
    "<b> To concatenate the various files </b> <br> \n",
    "dataframe = pd.concat([df ,df1, df2, df3, df4, df5, df6, df7, df8]) <br>\n",
    "dataframe <br>\n",
    "\n",
    "<b> Data Cleaning </b> <br> \n",
    "The collectible name, brand and artist information are not easily scrapable from the site, hence the link is used to extract this information. The trend is the past year is observed that the collectible name is separated by a preposition and the artist and brand name are separated by 'x'. <br>\n",
    "After all the cleaning on python, a manual check should be done because this may not always render the right results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa26541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing split value columns\n",
    "new = dataframe[\"Link\"].str.split('by|From|With|At|On|For',expand=True)\n",
    "  \n",
    "# Making separate last name column from new data frame\n",
    "dataframe[\"Collectibles\"]= new[0]\n",
    "dataframe[\"Artist/Company\"]= new[1]\n",
    "\n",
    "# Cleaning the hypen and html from the 'Artist/Company'\n",
    "dataframe['Artist/Company'] = dataframe['Artist/Company'].str.replace(\"-\", \" \")\n",
    "dataframe['Artist/Company'] = dataframe['Artist/Company'].str.replace(\".html\", \"\")\n",
    "\n",
    "# Storing split value columns\n",
    "new1 = dataframe[\"Artist/Company\"].str.split(' x ',expand=True)\n",
    "  \n",
    "# Making separate last name column from new data frame\n",
    "dataframe[\"Artist\"] = new1[0]\n",
    "\n",
    "dataframe[\"Company\"] = new1[1]\n",
    "\n",
    "# Display dataframe\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2f46d",
   "metadata": {},
   "source": [
    "<b> Extraction from individual link </b> <br>\n",
    "Each link has the description of the product such as the size, color, edition, material and tags. We need to extract this html text of the description and then use that to find the attributes we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1814833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using each link to extract the html text embedded within\n",
    "text = []\n",
    "link = []\n",
    "\n",
    "\n",
    "try:\n",
    "    for i in list(dataframe['Link'].iloc[1:50]):\n",
    "        url = i\n",
    "        x = urlopen(url)\n",
    "        start = time.time()\n",
    "        new = x.read()\n",
    "        soup = bs.BeautifulSoup(new,\"lxml\", multi_valued_attributes = None)\n",
    "        text.append(soup.find_all('p'))\n",
    "        sleep(randint(90,150))\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        print(str(elapsed))\n",
    "        link.append(i)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "    \n",
    "vinyl = {'Text': text, 'Link': link}\n",
    "dataframe_text = pd.DataFrame(vinyl)\n",
    "display(dataframe_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925820a",
   "metadata": {},
   "source": [
    "<b> Defining price and size in a function using regex to extract information from text </b> <br>\n",
    "\n",
    "Price starts with a dollar sign can be whole numbers, decimals, separated by a comma or a combination of both eg <br> \n",
    "\n",
    "1. 185.93 - https://www.vinylpulse.com/2021/07/kid-katana-005-renegade-leader-by-2petalrose.html <br>\n",
    "2. 3,850 - https://www.vinylpulse.com/2021/06/mickey-mouse-mosaic-art-style-by-disney-x-milk.html <br>  \n",
    "3. 135 - https://www.vinylpulse.com/2021/06/it-bear-bob-v2-by-milkboy-toys.html <br>\n",
    "\n",
    "Regex breakdown:\\$[0-9]*[,]*[0-9]*[.]*[0-9]{2} <br>\n",
    "- \\$ - in regex '$' is a special character, hence to nullify that, in the code it is preceeded by a \\ <br>\n",
    "- [0-9]* - any number of digits, * signifying that zero or more instances <br>\n",
    "- [,]*[0-9]* - digits can/cannot be followed by a comma (hence *), which can be followed by some digits if comma is present <br>\n",
    "- [.]*[0-9]{2} - post the decimal, if there is (hence *), two additional digits must be there (trend in VP) <br>\n",
    "\n",
    "Size on the website is followed by the inch sign ″ and might be a decimal <br>\n",
    "\n",
    "Regex breakdown: \\d*?\\.?\\d*″ <br>\n",
    "- \\d*?\\.?\\d*″ - any number of digits, optional (hence ?) <br>\n",
    "- \\.? - decimal point is optional <br>\n",
    "- \\d*″ - any number of digits followed by ″ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859484a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(Text):\n",
    "    text1 = str(Text)\n",
    "    price = re.findall(r'\\$[0-9]*[,]*[0-9]*[.]*[0-9]{2}', text1)\n",
    "    return price\n",
    "\n",
    "def size(Text):\n",
    "    text2 = str(Text)\n",
    "    size =  re.findall(r'\\d*?\\.?\\d*″', text2)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f5195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying function to the dataframe\n",
    "dataframe_text['Price'] = dataframe_text['Text'].apply(price)\n",
    "dataframe_text['Size'] = dataframe_text['Text'].apply(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bde3c9",
   "metadata": {},
   "source": [
    "Data cleaning for price and size column as the apply function renders a list that is not usable for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_text['Price'] = dataframe_text['Price'].astype(str).str.replace(r'\\[|\\]', '')\n",
    "dataframe_text['Price'] = dataframe_text['Price'].str.replace(',', '')\n",
    "dataframe_text['Price'] = dataframe_text['Price'].str.replace(\"$\", \"\")\n",
    "dataframe_text['Price'] = dataframe_text['Price'].str.replace(\"'\", \"\")\n",
    "\n",
    "dataframe_text['Size'] = dataframe_text['Size'].astype(str).str.replace(r'\\[|\\]', '')\n",
    "dataframe_text['Size'] = dataframe_text['Size'].astype(str).str.replace(r'″', '')\n",
    "dataframe_text['Size'] = dataframe_text['Size'].astype(str).str.replace(r\"'\", '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1646c",
   "metadata": {},
   "source": [
    "<b> Manual Data Cleaning </b> <br>\n",
    "The above code removes all the unneccesary characters, however a manual clean up is required in case there are instances where there are multiple prices and sizes, a call needs to be made whether to duplicate the row or to eliminate the row with the smaller/larger price, smaller/larger size. After the manual clean up, the price and size column need to be converted to float type to enable data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_text['Price'] = pd.to_numeric(dataframe_text['Price'])\n",
    "dataframe_text['Size'] = pd.to_numeric(dataframe_text['Size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab55c6",
   "metadata": {},
   "source": [
    "Tags are the important key words for each link which start with 'tag' with text embedded between <a?> \n",
    "The following code identifies the entire block within tag, then splits it with a comma, filters out the html link and 'tag' and appends them under the column 'Tags' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate tags\n",
    "textlist = dataframe_text['Text'].str.findall(r'rel=\"tag\">.*</a>').str.join(\", \").fillna('')\n",
    "\n",
    "#remove html tags and links\n",
    "tags = []\n",
    "innertags = []\n",
    "textlistsplit =[x.split(',') for x in textlist]\n",
    "for i in textlistsplit:\n",
    "    for j in i:\n",
    "        j = re.sub(r'rel=\"tag\">', '', j)\n",
    "        j = re.sub(r'</a>', '' ,j)\n",
    "        j = re.sub(r'<a href=\"http\\S+', '', j)\n",
    "        j = j.lstrip()\n",
    "        innertags.append(j)\n",
    "    tags.append(innertags)\n",
    "    innertags = []\n",
    "\n",
    "add tags as new column\n",
    "dataframe_text['Tags'] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7198f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning for tags\n",
    "dataframe_text['Tags'] = dataframe_text['Tags'].astype(str).str.replace(r'\\[|\\]', '')\n",
    "dataframe_text['Tags'] = dataframe_text['Tags'].astype(str).str.replace(r\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all of these links using concat in panda and then 'Link' will be used iteratively to extract price\n",
    "#size and tagged information\n",
    "\n",
    "dataframe_link = pd.concat([dataframe_text ,dataframe_text1, dataframe_text2, dataframe_text3, dataframe_text4])\n",
    "dataframe_link\n",
    "\n",
    "#Save this as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two main csv files by 'link'\n",
    "\n",
    "#merge\n",
    "df_comp = pd.merge(dataframe, dataframe_link, on = 'Link')\n",
    "df_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f030ce",
   "metadata": {},
   "source": [
    "#### Basic Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac80397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96929f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking columns where 'Price' is not NA\n",
    "\n",
    "df_comp = df_comp[frame['Price'].notna()]\n",
    "\n",
    "# to verify\n",
    "df_comp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique companies in the dataset\n",
    "print(df_comp['Company'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of companies, then finding the top 11\n",
    "frequency_co = frame['Company'].value_counts().nlargest(11)\n",
    "frequency_co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the list of top 11 companies\n",
    "comp_name = ['Unbox Industries','Mighty Jaxx','Martian Toys','Uvd Toys', 'Clutter','Superplastic','Fools Paradise','3Dretro','Kidrobot','Allrightsreserved','Sank Toys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the average price from the list of competitors\n",
    "comp_mean = frame[frame.Company.isin(comp_name)].groupby('Company')['Price'].mean().nlargest(11)\n",
    "print(comp_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the max price from the list of competitors\n",
    "comp_mean = frame[frame.Company.isin(comp_name)].groupby('Company')['Price'].max().nlargest(11)\n",
    "print(comp_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the min price from the list of competitors\n",
    "comp_mean = frame[frame.Company.isin(comp_name)].groupby('Company')['Price'].min().nlargest(11)\n",
    "print(comp_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of top 20 artists\n",
    "frequency_art = frame['Artist'].value_counts().nlargest(20)\n",
    "frequency_art"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
